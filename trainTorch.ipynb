{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "1.10.0+cu113\n",
      "11.3\n",
      "0.11.1+cu113\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.hub\n",
    "import random\n",
    "\n",
    "import torchvision\n",
    "import torchvision.utils\n",
    "from torchvision.models import detection\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.ssd import SSD\n",
    "\n",
    "import dataset as data\n",
    "import utils as utils\n",
    "import pickle\n",
    "import attackMethods as am\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda_version)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "dataset = data.AdversarialDataset((640,640)) # todo: use resize to pull picture in batch\n",
    "\n",
    "loss = am.lossObjectness\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "train_loader  = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "#device = torch.device(\"cuda:0\")\n",
    "#'cuda:0' or 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Степан\\Desktop\\учеба\\Диплом\\FromUbuntu\\diplomfile\\dataset.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return (torch.tensor(image).permute(2, 0, 1).float() / 255)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "transpose() received an invalid combination of arguments - got (tuple), but expected one of:\n * (int dim0, int dim1)\n * (name dim0, name dim1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\4689~1\\AppData\\Local\\Temp/ipykernel_5276/3072178319.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[0mattackedIm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[0mclearPredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattackedImage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0maugmented_patch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maugmentations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, imgs, size, augment, profile)\u001b[0m\n\u001b[0;32m    476\u001b[0m             \u001b[0mfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_suffix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# image in CHW\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 478\u001b[1;33m                 \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# reverse dataloader .transpose(2, 0, 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    479\u001b[0m             \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# enforce 3ch input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# HWC\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: transpose() received an invalid combination of arguments - got (tuple), but expected one of:\n * (int dim0, int dim1)\n * (name dim0, name dim1)\n"
     ]
    }
   ],
   "source": [
    "#train on pictures from dset (pictures with persons in personIndex list)\n",
    "\n",
    "model.eval()\n",
    "model = model.float().to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "patch = am.generatePatch()\n",
    "#patch = am.patchFromImage(\"meme.jpg\")\n",
    "patch = patch.to(device)\n",
    "patch.requires_grad = True\n",
    "\n",
    "TVCoeff = 0.000000001\n",
    "GradRate = 0.03\n",
    "\n",
    "augmentations = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ColorJitter(brightness=0.4, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "        torchvision.transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 0.3)),\n",
    "        torchvision.transforms.RandomPerspective(distortion_scale=0.2, p=1.0),\n",
    "        torchvision.transforms.RandomRotation(degrees=(-30, 30)),])\n",
    "\n",
    "#personIndex = []\n",
    "\n",
    "epoches = 1\n",
    "\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    imageCounter = 0\n",
    "    for image, label in train_loader:\n",
    "        imageCounter += batch_size\n",
    "        for k in range(1):\n",
    "            cv2.imshow(\"patch\", cv2.cvtColor(cv2.resize(data.TenToIm(patch.clone().detach().to(torch.device(\"cpu\"))/255), (300,300)), cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            attackedImage = []\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            for im in image:\n",
    "                attackedImage.append(data.ImToTen(im).to(device))\n",
    "\n",
    "            #for attackedIm in attackedImage:\n",
    "            #    attackedIm.requires_grad = True\n",
    "\n",
    "            with torch.no_grad():\n",
    "                clearPredict = model(attackedImage)\n",
    "\n",
    "            augmented_patch = augmentations(patch)\n",
    "\n",
    "            for i in range(len(attackedImage)):\n",
    "                for l in label[i]:\n",
    "                    attackedImage[i] = am.setPatch(attackedImage[i], augmented_patch, l, 0.2, device) \n",
    "\n",
    "            predict = model(attackedImage)\n",
    "\n",
    "            costs = []\n",
    "\n",
    "            for i in range(len(clearPredict)):\n",
    "                cost = loss(clearPredict[i], predict[i]) + TVCoeff * am.TV(patch, device)\n",
    "                if cost < 0.1:\n",
    "                    continue\n",
    "                try:\n",
    "                    grad = torch.autograd.grad(cost, patch, retain_graph=False, create_graph=False,  allow_unused=True)[0]\n",
    "                    if grad != None:\n",
    "                        patch = patch - GradRate*grad.sign()\n",
    "                except:\n",
    "                    pass\n",
    "                costs.append(cost.detach().cpu())\n",
    "            \n",
    "\n",
    "            print(\"ep:\", epoch,\"epoch_progress:\", imageCounter/len(dataset), \"loss:\", np.mean(np.asarray(costs)))\n",
    "\n",
    "            for attackedIm in attackedImage:\n",
    "                attackedIm.detach()\n",
    "\n",
    "            key = cv2.waitKey(1)\n",
    "            if key & 0xFF == ord('q'):\n",
    "                cv2.destroyAllWindows() \n",
    "                utils.SavePatch(patch.cpu(), \"patch - meme\")\n",
    "                raise StopExecution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'utils' has no attribute 'ShowPatch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\4689~1\\AppData\\Local\\Temp/ipykernel_5276/2419076859.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mShowPatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'utils' has no attribute 'ShowPatch'"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "cv2.destroyAllWindows() \n",
    "utils.ShowPatch(patch.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image, label = dataset[personIndex[7]]\n",
    "#network parameters requar. grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch = utils.LoadPatch(\"patch - meme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.SavePatch(patch.cpu(), \"patch - meme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('0_4784.pickle', 'wb') as f:\n",
    "    pickle.dump(personIndex, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('0_4784.pickle', 'rb') as f:\n",
    "    personIndex = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0  loss: tensor(1.1819, grad_fn=<AddBackward0>)\n",
      "ep: 1  loss: tensor(1.1061, grad_fn=<AddBackward0>)\n",
      "ep: 2  loss: tensor(1.0346, grad_fn=<AddBackward0>)\n",
      "ep: 3  loss: tensor(0.9632, grad_fn=<AddBackward0>)\n",
      "ep: 4  loss: tensor(0.8850, grad_fn=<AddBackward0>)\n",
      "ep: 5  loss: tensor(0.8303, grad_fn=<AddBackward0>)\n",
      "ep: 6  loss: tensor(0.7588, grad_fn=<AddBackward0>)\n",
      "ep: 7  loss: tensor(0.6875, grad_fn=<AddBackward0>)\n",
      "ep: 8  loss: tensor(0.6162, grad_fn=<AddBackward0>)\n",
      "ep: 9  loss: tensor(0.5449, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "image, label = dataset[personIndex[7]]\n",
    "patch = utils.TrainOneImage(model, loss, image, label, epoches=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clear loss: tensor(1.1095, grad_fn=<MulBackward0>) Attacked loss: tensor(1.1095, grad_fn=<MulBackward0>)\n",
      "Clear loss: tensor(1.2138, grad_fn=<MulBackward0>) Attacked loss: tensor(1.1075, grad_fn=<MulBackward0>)\n",
      "Clear loss: tensor(1.1265, grad_fn=<MulBackward0>) Attacked loss: tensor(1.1043, grad_fn=<MulBackward0>)\n",
      "Clear loss: tensor(1.1616, grad_fn=<MulBackward0>) Attacked loss: tensor(1.2186, grad_fn=<MulBackward0>)\n",
      "Clear loss: tensor(1.1064, grad_fn=<MulBackward0>) Attacked loss: tensor(1.1702, grad_fn=<MulBackward0>)\n",
      "Clear loss: tensor(1.8352, grad_fn=<MulBackward0>) Attacked loss: tensor(1.1531, grad_fn=<MulBackward0>)\n",
      "Clear loss: tensor(1.4608, grad_fn=<MulBackward0>) Attacked loss: tensor(1.1437, grad_fn=<MulBackward0>)\n",
      "Clear loss: tensor(1.1680, grad_fn=<MulBackward0>) Attacked loss: tensor(1.1739, grad_fn=<MulBackward0>)\n",
      "Clear loss: tensor(1.1299, grad_fn=<MulBackward0>) Attacked loss: tensor(1.1785, grad_fn=<MulBackward0>)\n",
      "Clear loss: tensor(1.6535, grad_fn=<MulBackward0>) Attacked loss: tensor(1.6535, grad_fn=<MulBackward0>)\n",
      "Clear loss: tensor(1.3564, grad_fn=<MulBackward0>) Attacked loss: tensor(1.1033, grad_fn=<MulBackward0>)\n",
      "Clear loss: tensor(1.1515, grad_fn=<MulBackward0>) Attacked loss: tensor(1.1492, grad_fn=<MulBackward0>)\n",
      "Clear loss: tensor(1.1028, grad_fn=<MulBackward0>) Attacked loss: tensor(1.1296, grad_fn=<MulBackward0>)\n",
      "Clear loss: tensor(1.1026, grad_fn=<MulBackward0>) Attacked loss: tensor(1.1651, grad_fn=<MulBackward0>)\n",
      "Clear loss: tensor(1.1539, grad_fn=<MulBackward0>) Attacked loss: tensor(1.1444, grad_fn=<MulBackward0>)\n",
      "Clear loss: tensor(1.1597, grad_fn=<MulBackward0>) Attacked loss: tensor(1.1168, grad_fn=<MulBackward0>)\n",
      "Clear loss: tensor(1.1694, grad_fn=<MulBackward0>) Attacked loss: tensor(1.1131, grad_fn=<MulBackward0>)\n",
      "Clear loss: tensor(1.5332, grad_fn=<MulBackward0>) Attacked loss: tensor(1.1434, grad_fn=<MulBackward0>)\n",
      "Clear loss: tensor(1.1606, grad_fn=<MulBackward0>) Attacked loss: tensor(1.1072, grad_fn=<MulBackward0>)\n",
      "Clear loss: tensor(1.1065, grad_fn=<MulBackward0>) Attacked loss: tensor(1.1208, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    image, label = dataset[personIndex[i]]\n",
    "    utils.CheckPatch(patch, image, label, model, device, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2d769b0611aaa6c068c0119a1ada538e3a78d93ac59b96a484ef3c5a008ee0c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
